#Sidd Lakshman
#Basic Neural Network which can identify an object containing two characteristics
#In this case, object = flower (red = 0/blue = 1) and characteristics = length, width

%matplotlib inline
from matplotlib import pyplot as plt
import numpy as np

#data := [length, width, type]
data = [[3, 1.5, 1],
        [2, 1, 0],
        [4, 1.5, 1],
        [3, 1, 0],
        [3.5, .5, 1],
        [2, .5, 0],
        [5.5, 1, 1],
        [1, 1, 0]]

#should ideally predict type 1
unknownFlower = [4.5, 1]

#network
#     o               type
#    / \             /    \
#   o   o       length  width

w1 = np.random.randn()
w2 = np.random.randn()
b = np.random.randn()

#define sigmoid/derivative
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_p(x):
    return sigmoid(x)*(1-sigmoid(x))


# training loop
learning_rate = 0.50
costs = []
for i in range(100000):
    ri = np.random.randint(len(data))
    point = data[ri]
    z = point[0] * w1 + point[1] * w2 + b
    pred = sigmoid(z)
    target = point[2]

    # cost of our function
    cost = np.square(pred - target)
    dcost_pred = 2 * (pred - target)
    dpred_dz = sigmoid_p(z)

    costs.append(cost)

    # derivatives wrt variables
    dz_dw1 = point[0]
    dz_dw2 = point[1]
    dz_db = 1

    # differentials of our cost wrt variables
    dcost_dz = dcost_pred * dpred_dz
    dcost_dw1 = dcost_dz * dz_dw1
    dcost_dw2 = dcost_dz * dz_dw2
    dcost_db = dcost_dz * dz_db

    # adjustments using our differentials and learning rate
    w1 = w1 - learning_rate * dcost_dw1
    w2 = w2 - learning_rate * dcost_dw2
    b = b - learning_rate * dcost_db

for i in range(len(data)):
    point = data[i]
    print(point)
    z = point[0] * w1 + point[1] * w2 + b
    pred = sigmoid(z)
    print("pred: {}".format(pred))

z = unknownFlower[0]*w1 + unknownFlower[1]*w2 + b
pred = sigmoid(z)
pred